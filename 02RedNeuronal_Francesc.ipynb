{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP93zAsw7bcveJc68YO2gbC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"sNlTzw2-RMAn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOlWjIU4uEyV","executionInfo":{"status":"ok","timestamp":1688061925743,"user_tz":-120,"elapsed":3391,"user":{"displayName":"Francesc Polls Agell","userId":"08524325674868416435"}},"outputId":"ab8925cb-7d53-4f4a-e699-158136be781e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import tensorflow as tf\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import os\n","from google.colab import files\n","from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"cMsuDjSJuHWF","executionInfo":{"status":"ok","timestamp":1688061925744,"user_tz":-120,"elapsed":7,"user":{"displayName":"Francesc Polls Agell","userId":"08524325674868416435"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5KFMqaIPROlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Ym4_i7QrROcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fileList = os.listdir('/content/drive/MyDrive/prova/dades')\n","csv_files = [file for file in fileList if file[-3:] == \"csv\"]\n","batch_size = 1000\n","csv_files=sorted(csv_files)\n","\n","# Crear una lista vacía para almacenar los DataFrames de cada archivo\n","dataframes = []\n","\n","# Leer cada archivo CSV y agregarlo a la lista de DataFrames\n","for archivo in csv_files:\n","\n","    df = pd.read_csv('/content/drive/MyDrive/prova/dades/'+archivo)\n","    df=df.loc[:, ['num_docks_available', 'Llocs','station_id','hour','dayofweek','month','Wind','Rain']]\n","    dataframes.append(df)\n","\n","# Combinar todos los DataFrames en uno solo\n","dataframe_final = pd.concat(dataframes, ignore_index=True)\n","\n","# Mostrar el DataFrame resultante\n","print(dataframe_final)"],"metadata":{"id":"9kbd9m_rPGKH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688061977436,"user_tz":-120,"elapsed":32516,"user":{"displayName":"Francesc Polls Agell","userId":"08524325674868416435"}},"outputId":"e621cb01-5063-4b6c-a185-f320aab8e14f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["         num_docks_available  Llocs  station_id  hour  dayofweek  month  Wind  \\\n","0                   5.500000   46.0           1   0.0          4    1.0     0   \n","1                   5.000000   46.0           1   1.0          4    1.0     0   \n","2                   5.000000   46.0           1   2.0          4    1.0     0   \n","3                   5.000000   46.0           1   3.0          4    1.0     0   \n","4                   5.000000   46.0           1   4.0          4    1.0     0   \n","...                      ...    ...         ...   ...        ...    ...   ...   \n","8819374            21.250000   22.0         519  18.0          4    9.0     0   \n","8819375            18.666667   22.0         519  19.0          4    9.0     0   \n","8819376            16.916667   22.0         519  20.0          4    9.0     0   \n","8819377            17.666667   22.0         519  21.0          4    9.0     0   \n","8819378            18.333333   22.0         519  22.0          4    9.0     0   \n","\n","         Rain  \n","0           1  \n","1           1  \n","2           1  \n","3           1  \n","4           1  \n","...       ...  \n","8819374     0  \n","8819375     0  \n","8819376     0  \n","8819377     0  \n","8819378     0  \n","\n","[8819379 rows x 8 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Cargar los datos en lotes y realizar transformaciones\n","# Cargar los datos en lotes y realizar transformaciones\n","def load_and_process_data(csv_file):\n","  data=csv_file\n","    # Cargar el lote de datos desde el archivo CSV\n","  data.dropna(subset=['num_docks_available', 'Llocs','station_id','hour','dayofweek','month','Wind','Rain'], inplace=True)\n","  data['porcio']=(data['num_docks_available']/data['Llocs'])\n","  data['festa']=0\n","  data['festa']=data['festa'].where(data['dayofweek']>5,1)\n","  data['festa']=data['festa'].where(data['month']==8,1)\n","  data['winter']=0\n","  data['winter']=data['winter'].where(data['month']==12,1)\n","  data['winter']=data['winter'].where(data['month']==1,1)\n","  data['winter']=data['winter'].where(data['month']==2,1)\n","\n","\n","  data['spring']=0\n","  data['spring']=data['spring'].where(data['month']==3,1)\n","  data['spring']=data['spring'].where(data['month']==4,1)\n","  data['spring']=data['spring'].where(data['month']==5,1)\n","  data['summer']=0\n","  data['summer']=data['summer'].where(data['month']==6,1)\n","  data['summer']=data['summer'].where(data['month']==7,1)\n","  data['summer']=data['summer'].where(data['month']==8,1)\n","\n","  data['fall']=0\n","  data['fall']=data['fall'].where(data['month']==9,1)\n","  data['fall']=data['fall'].where(data['month']==10,1)\n","  data['fall']=data['fall'].where(data['month']==11,1)\n","  data.dropna(subset=['porcio'], inplace=True)\n","    # Realizar las transformaciones necesarias en los datos\n","    # Por ejemplo, codificar variables categóricas, escalar valores numéricos, etc.\n","\n","    # Dividir los datos en características (X) y etiquetas (Y)\n","  X = data[['station_id','hour','festa','month','Rain']]\n","  X = data[['station_id','hour','festa','winter','spring','summer','fall','Rain']]\n","    # X = data[['station_id','hour','dayofweek','Rain_Lectura']]\n","\n","  Y = (data['porcio'])\n","\n","    # Devolver los datos procesados\n","  return X, Y\n","def load_and_process_data2(csv_file):\n","  data = pd.read_csv(csv_file)\n","    # Cargar el lote de datos desde el archivo CSV\n","    # Realizar las transformaciones necesarias en los datos\n","    # Por ejemplo, codificar variables categóricas, escalar valores numéricos, etc.\n","  data['festa']=0\n","  data['festa']=data['festa'].where(data['dayofweek']>5,1)\n","  data['festa']=data['festa'].where(data['month']==8,1)\n","  data['winter']=0\n","  data['winter']=data['winter'].where(data['month']==12,1)\n","  data['winter']=data['winter'].where(data['month']==1,1)\n","  data['winter']=data['winter'].where(data['month']==2,1)\n","\n","\n","  data['spring']=0\n","  data['spring']=data['spring'].where(data['month']==3,1)\n","  data['spring']=data['spring'].where(data['month']==4,1)\n","  data['spring']=data['spring'].where(data['month']==5,1)\n","  data['summer']=0\n","  data['summer']=data['summer'].where(data['month']==6,1)\n","  data['summer']=data['summer'].where(data['month']==7,1)\n","  data['summer']=data['summer'].where(data['month']==8,1)\n","\n","  data['fall']=0\n","  data['fall']=data['fall'].where(data['month']==9,1)\n","  data['fall']=data['fall'].where(data['month']==10,1)\n","  data['fall']=data['fall'].where(data['month']==11,1)\n","    # Dividir los datos en características (X) y etiquetas (Y)\n","  X = data[['station_id','hour','festa','month','Rain']]\n","  X = data[['station_id','hour','festa','winter','spring','summer','fall','Rain']]\n","\n","    # Devolver los datos procesados\n","  return X\n","\n","X_train, Y_train = load_and_process_data(dataframe_final)\n","\n","# Escalar los valores numéricos en el rango [0, 1]\n","scaler = MinMaxScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","\n","# Crear un modelo de red neuronal\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(256, activation='relu', input_shape=(8,)),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","# Definir el optimizador Adam con una tasa de aprendizaje variable\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=0.001,\n","    decay_steps=1000,\n","    decay_rate=0.9\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","# Compilar el modelo\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n","\n","# Entrenar el modelo en los conjuntos de entrenamiento\n","model.fit(X_train_scaled, Y_train, epochs=10, batch_size=32)\n","\n","# Realizar predicciones con el modelo entrenado\n","new_data = load_and_process_data2('/content/drive/MyDrive/prova/test/submission.csv')\n","new_data_scaled = scaler.transform(new_data)\n","\n","predictions = model.predict(new_data_scaled)\n","ids = new_data.index.values\n","\n","# Crear un DataFrame con las predicciones y los IDs\n","df = pd.DataFrame({'index': ids, 'percentage_docks_available': predictions[:, 0]})\n","\n","# Definir la ruta y el nombre del archivo CSV\n","csv_file = 'predictions.csv'\n","\n","# Guardar el DataFrame en el archivo CSV\n","df.to_csv(csv_file, index=False)\n","files.download('/content/'+csv_file)\n"],"metadata":{"id":"-MHp7oBKrGLY","colab":{"base_uri":"https://localhost:8080/","height":395},"outputId":"bcf5c1b8-d21d-43e6-d776-70e5f9c0a0d5","executionInfo":{"status":"ok","timestamp":1688069210566,"user_tz":-120,"elapsed":7233159,"user":{"displayName":"Francesc Polls Agell","userId":"08524325674868416435"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","275555/275555 [==============================] - 708s 3ms/step - loss: 0.0779\n","Epoch 2/10\n","275555/275555 [==============================] - 712s 3ms/step - loss: 0.0766\n","Epoch 3/10\n","275555/275555 [==============================] - 703s 3ms/step - loss: 0.0766\n","Epoch 4/10\n","275555/275555 [==============================] - 703s 3ms/step - loss: 0.0766\n","Epoch 5/10\n","275555/275555 [==============================] - 704s 3ms/step - loss: 0.0766\n","Epoch 6/10\n","275555/275555 [==============================] - 722s 3ms/step - loss: 0.0766\n","Epoch 7/10\n","275555/275555 [==============================] - 727s 3ms/step - loss: 0.0766\n","Epoch 8/10\n","275555/275555 [==============================] - 737s 3ms/step - loss: 0.0766\n","Epoch 9/10\n","275555/275555 [==============================] - 730s 3ms/step - loss: 0.0766\n","Epoch 10/10\n","275555/275555 [==============================] - 735s 3ms/step - loss: 0.0766\n","1719/1719 [==============================] - 2s 1ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_bdbcfbb6-a28b-46bf-8ef9-e2723417690a\", \"predictions.csv\", 887267)"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"t7TMvQ-KYnGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fT7ydr4DYDCP"},"execution_count":null,"outputs":[]}]}